cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
names <- c("lm", "ctree", "rpart")
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(sexo ~ . - bdi - edad, data = dep_data[fold_ids != i, ])
cart <- rpart(sexo ~ . - bdi - edad, data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
predict(gen_ctree, newdata = dep_data, type = "prob")
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
cv_preds
# do the cross validation
lmods <- ctrees <- list() # for saving fitted models (optional, for later use)
cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
names <- c("lm", "ctree", "rpart")
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
cv_preds
# do the cross validation
lmods <- ctrees <- list() # for saving fitted models (optional, for later use)
cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
cv_preds
names <- c("lm", "ctree", "rpart")
# do the cross validation
lmods <- ctrees <- list() # for saving fitted models (optional, for later use)
cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
names(cv_preds) <- c("lm", "ctree", "rpart")
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
cv_preds
# Evaluate accuracy:
cv_preds - dep_data$bdi
# Evaluate accuracy:
head(cv_preds - dep_data$bdi)
mean((cv_preds - dep_data$bdi)^2)
var(dep_data)
var(dep_data)
var(dep_data$bdi)
colMeans((cv_preds - dep_data$bdi)^2)
sapply((cv_preds - dep_data$bdi)^2, sd)
# Evaluate accuracy:
head(cv_preds - dep_data$bdi)
colMeans((cv_preds - dep_data$bdi)^2)
sapply((cv_preds - dep_data$bdi)^2, sd)
cv_preds(cv_preds - dep_data$bdi)^2
(cv_preds - dep_data$bdi)^2
class((cv_preds - dep_data$bdi)^2)
class(cv_preds)
?apply
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd)
colMeans((cv_preds - dep_data$bdi)^2)
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd)
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd) / sqrt(nrow(dep_data))
# Evaluate accuracy:
head(cv_preds - dep_data$bdi)
colMeans((cv_preds - dep_data$bdi)^2)
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd)
# calculate standard errors:
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd) / sqrt(nrow(dep_data))
# Evaluate accuracy:
head(cv_preds - dep_data$bdi)
boxplot(cv_preds - dep_data$bdi)
colMeans((cv_preds - dep_data$bdi)^2)
colMeans((cv_preds - dep_data$bdi)^2) # mean sqaured errors
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd)
apply((cv_preds - dep_data$bdi)^2, MARGIN = 2, FUN = sd) / sqrt(nrow(dep_data))
# Evaluate accuracy:
errors <- cv_preds - dep_data$bdi
# Evaluate accuracy:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
?t.test
?anova
?stack
stack(errors)
errors_aov <- stack(errors)
head(errors_aov)
aov(values ~ ind, data = errors_aov)
errors_aov_dat <- stack(errors)
head(errors_aov)
errors_aov <- aov(values ~ ind, data = errors_aov_dat)
summary(errors_aov)
TukeyHSD(errors_aov)
# Evaluate accuracy:
errors <- (cv_preds - dep_data$bdi)^%2
# Evaluate accuracy:
errors <- (cv_preds - dep_data$bdi)^2
boxplot(errors)
colMeans(errors)
apply(errors, MARGIN = 2, FUN = sd)
apply(errors, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
errors_aov_dat <- stack(errors)
head(errors_aov)
errors_aov_dat <- stack(errors)
# Evaluate accuracy:
errors <- (cv_preds - dep_data$bdi)^2
errors
errors_aov_dat <- stack(errors)
stack(errors)
head(errors)
stack(errors)
# Evaluate accuracy:
errors <- (cv_preds - dep_data$bdi)
errors_aov_dat <- stack(errors)
# Evaluate accuracy:
errors <- (cv_preds - dep_data$bdi)^2
errors_aov_dat <- stack(errors)
errors_aov_dat <- stack(errors)^2
# Evaluate accuracy:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
errors_aov_dat <- stack(errors)^2
errors_aov_dat <- stack(errors)
errors_aov <- aov(values^2 ~ ind, data = errors_aov_dat)
summary(errors_aov)
TukeyHSD(errors_aov)
# Evaluate accuracy:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
errors_aov_dat <- stack(errors)
head(errors_aov)
errors_aov_dat <- stack(errors)
errors_avo_dat
errors_aov_dat
head(errors_aov_dat)
summary(errors_aov)
TukeyHSD(errors_aov)
# Assess mean squared errorEvaluate accuracy:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
plot(dep_data)
# Fit linear model:
dep_lm <- lm(bdi ~ ., data = dep_data)
dep_lm # print(dep_lm) would yield same result
summary(dep_lm) # gives significance tests
plot(dep_lm) # for checking assumptions
plot(dep_ctree, type = "simple")
print(dep_ctree)
library("rpart")
dep_cart <- rpart(bdi ~ ., data = dep_data)
plot(dep_cart)
text(dep_cart)
# Fit logistic regression:
gen_glm <- glm(sexo ~ . - bdi - edad, data = dep_data, family = "binomial")
gen_glm
summary(gen_glm)
plot(gen_glm)
plot(gen_cart)
text(gen_cart)
# Use of predict function:
predict(dep_lm, newdata = dep_data)
predict(dep_ctree, newdata = dep_data)
predict(dep_cart, newdata = dep_data)
predict(gen_glm, newdata = dep_data)
predict(gen_ctree, newdata = dep_data)
predict(gen_ctree, newdata = dep_data, type = "prob")
predict(gen_cart, newdata = dep_data)
# generate k folds
k <- 10
fold_ids <- rep(1:k, length.out = nrow(dep_data))
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
dim(dep_data[fold_ids != 1, ]) # training data for fold 1
dim(dep_data[fold_ids == 1, ]) # test data for fold 1
dim(dep_data[fold_ids != 9, ]) # training data for fold 9
dim(dep_data[fold_ids == 9, ]) # test data for fold 9
# do the cross validation
lmods <- ctrees <- list() # for saving fitted models (optional, for later use)
cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
names(cv_preds) <- c("lm", "ctree", "rpart")
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
# Assess mean squared errorEvaluate accuracy:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
# Assess mean squared error:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
# Perform anova:
errors_aov_dat <- stack(errors)
# Perform anova:
errors_aov_dat <- stack(errors)
head(errors_aov_dat)
errors_aov <- aov(values^2 ~ ind, data = errors_aov_dat)
summary(errors_aov)
TukeyHSD(errors_aov)
##
## Example 3: Fit trees to multilevel data
##
UKMH_data <- read.table("UK_MH mimic data.txt")
dim(UKMH_data)
head(UKMH_data)
UKMH_tree <- lmertree(outcome ~ 1 | cluster_id | age + gender + emotional +
autism + impact + conduct, data = UKMH_data)
##
## Fit trees to multilevel data
##
library("glmertree")
UKMH_data <- read.table("UK_MH mimic data.txt")
dim(UKMH_data)
head(UKMH_data)
UKMH_tree <- lmertree(outcome ~ 1 | cluster_id | age + gender + emotional +
autism + impact + conduct, data = UKMH_data)
plot(UKMH_tree)
resids <- residuals(UKMH_tree)
plot(x = factor(UKMH_data$cluster_id), y = resids,
xlab = "cluster indicator", ylab = "residuals",
boxwex = .5, cex.lab=.75, cex.axis = .75)
# check assumptions:
resids <- residuals(UKMH_tree)
plot(x = factor(UKMH_data$cluster_id), y = resids,
xlab = "cluster indicator", ylab = "residuals",
boxwex = .5, cex.lab=.75, cex.axis = .75)
preds <- predict(UKMH_tree)
scatter.smooth(x = preds, y = resids, xlab = "predicted values",
ylab = "residuals", cex = .5, cex.lab = .75, cex.axis = .75)
##
## Fit model-based trees
##
data(DepressionDemo)
?glmertree
##
## Fit model-based trees
##
data("DepressionDemo", package = "glmertree")
head(DepressionDemo)
lt <- lmertree(depression ~ treatment | cluster | age + anxiety + duration,
data = DepressionDemo)
print(lt)
##
## Fit model-based trees
##
data("DepressionDemo", package = "glmertree")
head(DepressionDemo)
lt <- lmtree(depression ~ treatment | age + anxiety + duration,
data = DepressionDemo)
print(lt)
##
## Fit model-based trees
##
data("DepressionDemo", package = "glmertree")
head(DepressionDemo)
# fit linear model tree
lt <- lmtree(depression ~ treatment | age + anxiety + duration,
data = DepressionDemo)
print(lt)
plot(lt)
# fit linear mixed-effects model tree:
lmert <- lmtree(depression ~ treatment | age + anxiety + duration,
data = DepressionDemo)
print(lmert)
plot(lmert)
predict(lmert, type = "response") # default behavior, type may also be "node"
predict(lmert) # default behavior, type may also be "node"
predict(lmert, newdata = DepressionDemo) # default behavior, type may also be "node"
head(predict(lmert, re.form = NA, newdata = DepressionDemo))
head(predict(lmert, newdata = DepressionDemo))
head(predict(lmert, re.form = NA, newdata = DepressionDemo))
# fit linear mixed-effects model tree:
lmert <- lmtree(depression ~ treatment | age + anxiety + duration,
data = DepressionDemo)
print(lmert)
plot(lmert)
head(predict(lmert, newdata = DepressionDemo))
head(predict(lmert, re.form = NA, newdata = DepressionDemo))
head(predict(lmert, re.form = NULL, newdata = DepressionDemo))
# fit linear mixed-effects model tree:
lmert <- lmtree(depression ~ treatment | age + anxiety + duration,
data = DepressionDemo)
plot(lmert)
# fit linear mixed-effects model tree:
lmert <- lmertree(depression ~ treatment | cluster | age + anxiety + duration,
data = DepressionDemo)
print(lmert)
plot(lmert)
head(predict(lmert, newdata = DepressionDemo))
head(predict(lmert, re.form = NA))
?load
# save results:
save(cv_preds, file = "cv_preds")
load("cv_preds")
?citation
citation()
citation
citation()
citation()
citation("partykit")
rm()
library("foreign")
library("foreign")
setwd("C:/Users/fokkemam/Desktop/Onderwijs Leiden/Bachelorthese project 2018-2019/B-these project Trees/Introductie R en beslisbomen")
library("foreign")
dep_data <- read.spss("data Carrillo et al.sav", to.data.frame = TRUE)
?read.spss
dep_data
## Inspect data:
head(dep_data)
dim(dep_data)
dep_data[2,3]
dep_data[2,]
dep_data[,3]
names(dep_data)
summary(dep_data)
dep_data[2,] <- NA
head(dep_data)
summary(dep_data)
dep_data <- read.spss("data Carrillo et al.sav", to.data.frame = TRUE)
plot(dep_data)
plot(dep_data[,1:10])
1:10
c(1,2,3,4,5,6,7,8,9,10)
c(1,2,3,4,5,6,7,8,9,10) == 1:10
c(1,2,3,4,5,6,7,8,9,10) == 1:9
sapply(dep_data, class)
# Correctly set class of variable:
dep_data$sexo <- factor(dep_data$sexo)
sapply(dep_data, class)
dep_data$sexo
?lm
# Fit linear model:
dep_lm <- lm(bdi ~ ., data = dep_data)
dep_lm
print(dep_lm)
summary(dep_lm) # gives significance tests
plot(dep_lm) # for checking assumptions
# Fit regression trees:
library("partykit")
# Fit regression trees:
library("partykit")
dep_ctree <- ctree(bdi ~ ., data = dep_data)
plot(dep_ctree)
plot(dep_ctree, type = "simple")
print(dep_ctree)
library("rpart")
dep_cart <- rpart(bdi ~ ., data = dep_data)
plot(dep_cart)
text(dep_cart)
dep_cart
# Fit logistic regression:
gen_glm <- glm(sexo ~ . - bdi - edad, data = dep_data, family = "binomial")
gen_glm
summary(gen_glm)
?carrillo
?pre:::carrillo
# Fit classification trees:
gen_ctree <- ctree(sexo ~ . - bdi - edad, data = dep_data)
plot(gen_ctree)
gen_cart <- rpart(sexo ~ . - bdi - edad, data = dep_data)
plot(gen_cart)
text(gen_cart)
gen_cart
# Use of predict function:
predict(dep_lm, newdata = dep_data)
predict(dep_ctree, newdata = dep_data)
predict(dep_cart, newdata = dep_data)
predict(gen_glm, newdata = dep_data)
predict(gen_ctree, newdata = dep_data)
predict(gen_glm, newdata = dep_data, type = "prob")
predict(gen_glm, newdata = dep_data, type = "response")
predict(gen_glm, newdata = dep_data, type = "response")
predict(gen_ctree, newdata = dep_data)
predict(gen_ctree, newdata = dep_data, type = "prob")
predict(gen_ctree, newdata = dep_data, type = "prob")[,2]
predict(gen_cart, newdata = dep_data)
# generate k folds for CV:
k <- 10
fold_ids <- rep(1:k, length.out = nrow(dep_data))
fold_ids
set.seed(42)
?sample
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
# generate k folds for CV:
k <- 10
fold_ids <- rep(1:k, length.out = nrow(dep_data))
fold_ids
set.seed(42)
fold_ids <- sample(fold_ids)
fold_ids
dim(dep_data[fold_ids != 1, ]) # training data for fold 1
dep_data[fold_ids != 1, ]
fold_ids
fold_ids != 1
fold_ids == 1
dim(dep_data[fold_ids != 1, ]) # training data for fold 1
dim(dep_data[fold_ids == 1, ]) # test data for fold 1
dim(dep_data[fold_ids != 9, ]) # training data for fold 9
dim(dep_data[fold_ids == 9, ]) # test data for fold 9
# perform CV:
lmods <- ctrees <- list() # for saving fitted models (optional, for later use)
cv_preds <- data.frame(matrix(NA, nrow = nrow(dep_data), ncol = 3))
names(cv_preds) <- c("lm", "ctree", "rpart")
cv_preds
i=1
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
lmods
ctrees
cart
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
for (i in 1:k) {
# Fit models:
lmods[[i]] <- lm(bdi ~ ., data = dep_data[fold_ids != i, ])
ctrees[[i]] <- ctree(bdi ~ ., data = dep_data[fold_ids != i, ])
cart <- rpart(bdi ~ ., data = dep_data[fold_ids != i, ])
# get predictions:
cv_preds$lm[fold_ids == i] <- predict(lmods[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$ctree[fold_ids == i] <- predict(ctrees[[i]],
newdata = dep_data[fold_ids == i,])
cv_preds$rpart[fold_ids == i] <-predict(cart,
newdata = dep_data[fold_ids == i,])
}
# save results:
save(cv_preds, file = "cv_preds")
load("cv_preds")
# Assess mean squared error:
errors <- cv_preds - dep_data$bdi
boxplot(errors)
colMeans(errors^2)
apply(errors^2, MARGIN = 2, FUN = sd)
apply(errors^2, MARGIN = 2, FUN = sd) / sqrt(nrow(errors))
